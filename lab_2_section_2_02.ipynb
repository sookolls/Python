{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e30264da",
   "metadata": {},
   "source": [
    "**Задача 1: Система управления библиотекой**\n",
    "\n",
    "Создайте систему управления библиотекой используя collections:\n",
    "\n",
    "1. Создайте namedtuple `Book` с полями: title, author, isbn, year\n",
    "2. Создайте namedtuple `Reader` с полями: name, reader_id, phone\n",
    "3. Используйте `defaultdict(list)` для хранения книг по жанрам\n",
    "4. Используйте `deque` для очереди читателей, ожидающих популярную книгу\n",
    "5. Используйте `Counter` для подсчета количества книг каждого автора\n",
    "6. Используйте `OrderedDict` для хранения истории выдачи книг (читатель -> список книг)\n",
    "7. Сериализуйте все данные в JSON и pickle форматы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b3c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict, deque, Counter, OrderedDict\n",
    "from typing import NamedTuple\n",
    "import json, pickle\n",
    "\n",
    "Book = namedtuple('Book', ['title', 'author', 'isbn', 'year'])\n",
    "all_books: list[Book] = [] \n",
    "book1: Book = Book(\"fairy tale\", 'Author1', '123', 1874) #добавляем книги\n",
    "all_books.append(book1)\n",
    "book2 = Book(\"The stories\", 'Author1', '345', 1901)\n",
    "all_books.append(book2)\n",
    "book3 = Book(\"fairy tale\", 'Author3', '567', 1991)\n",
    "all_books.append(book3)\n",
    "\n",
    "Reader = namedtuple('Reader', ['name', 'reader_id', 'phone'])\n",
    "reader1: Reader = Reader('ivan', '101','998765')#добавляем читателей\n",
    "reader2 = Reader('oleg', '102','445567')\n",
    "book_genre: dict[str, list[Book]] = defaultdict(list)\n",
    "book_genre[\"fairy tale\"].append(book1)\n",
    "book_genre[\"stories\"].append(book2)\n",
    "book_genre[\"fairy tale\"].append(book3)\n",
    "\n",
    "deque_readers: deque[str] = deque()\n",
    "deque_readers.append(reader1.name)#добавляем читателей в очередь\n",
    "deque_readers.append(reader2.name) \n",
    "\n",
    "author_counter : Counter[str] = Counter(book.author for book in all_books) #подсчёт количества книг авторов\n",
    "\n",
    "book_distribution: OrderedDict[str, list[Book]] = OrderedDict()\n",
    "book_distribution[reader1.name]=[book3, book2]\n",
    "book_distribution[reader2.name]=[book1]\n",
    "\n",
    "all_books_serializable = [book._asdict() for book in all_books]\n",
    "book_genre_serializable = {genre: [book._asdict() for book in books] for genre, books in book_genre.items()}\n",
    "deque_readers_serializable = list(deque_readers)\n",
    "author_counter_serializable = dict(author_counter)\n",
    "\n",
    "def serialize_history(history):\n",
    "    return {reader: [book._asdict() for book in books] for reader, books in history.items()}\n",
    "\n",
    "book_distribution_serializable = serialize_history(book_distribution)\n",
    "\n",
    "library_data = {\n",
    "    \"all_books\": all_books_serializable,\n",
    "    \"book_genre\": book_genre_serializable,\n",
    "    \"waiting_queue\": deque_readers_serializable,\n",
    "    \"author_counter\": author_counter_serializable,\n",
    "    \"history\": book_distribution_serializable\n",
    "}\n",
    "with open(\"library_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(library_data, f, ensure_ascii=False, indent=4)\n",
    "with open(\"library_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(library_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea1fdb",
   "metadata": {},
   "source": [
    "**Задача 2: Анализатор файловой системы**\n",
    "\n",
    "Создайте анализатор файловой системы используя os и sys:\n",
    "\n",
    "1. Создайте namedtuple `FileInfo` с полями: name, size, extension, modified_time\n",
    "2. Используйте `os.walk()` для обхода директории\n",
    "3. Используйте `os.path` функции для получения информации о файлах\n",
    "4. Используйте `Counter` для подсчета файлов по расширениям\n",
    "5. Используйте `defaultdict(list)` для группировки файлов по размеру (маленькие < 1MB, средние 1-100MB, большие > 100MB)\n",
    "6. Используйте `deque` для хранения последних 10 найденных файлов\n",
    "7. Выведите статистику используя `sys.getsizeof()` для подсчета памяти\n",
    "8. Сохраните результаты в JSON файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f939b12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\OneDrive\\Рабочий стол\\dev\\Python\n",
      "Статистика по расширениям:\n",
      ".ipynb: 4\n",
      ".json: 2\n",
      ".pkl: 1\n",
      ".txt: 1\n",
      ".swp: 1\n",
      "без расширения: 141\n",
      ".sample: 28\n",
      ".md: 1\n",
      ".py: 1\n",
      ".xml: 4\n",
      "Статистика по размерам:\n",
      "маленькие: 184 файлов\n",
      "Последние 10 найденных файлов:\n",
      "73c6abea7dde1d5b5bf27e2428247eb76eb9eb\n",
      "233b6982b2662f4ed0411ddd6459c6655893a1\n",
      "20c8dbb9cd7262eac3d982c95c8826a0ecc1a7\n",
      "main\n",
      "main\n",
      ".gitignore\n",
      "misc.xml\n",
      "vcs.xml\n",
      "workspace.xml\n",
      "profiles_settings.xml\n",
      "Размер списка file_info: 1656 байт\n",
      "Суммарный размер всех FileInfo объектов: 13248 байт\n",
      "Всего: 14904 байт\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple, Counter, defaultdict, deque\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "directory: str = r\"c:\\Users\\User\\OneDrive\\Рабочий стол\\dev\\Python\"\n",
    "FileInfo = namedtuple('FileInfo', ['name', 'size', 'extension', 'modified_time'])\n",
    "file_info: list[FileInfo] = []\n",
    "print(os.path.abspath(directory))\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for filename in files:\n",
    "        path: str = os.path.join(root, filename)\n",
    "        size: int = os.path.getsize(path)\n",
    "        name, extension = os.path.splitext(filename)\n",
    "        modified_time: float = os.path.getmtime(path)\n",
    "        file_info.append (FileInfo(name, size, extension, modified_time))\n",
    "extensions_counter: Counter[str] = Counter(inform.extension for inform in file_info)\n",
    "print(\"Статистика по расширениям:\")\n",
    "for ext, count in extensions_counter.items():\n",
    "    print(f\"{ext if ext else 'без расширения'}: {count}\")\n",
    "size_group: dict[str, list[FileInfo]] = defaultdict(list)\n",
    "for inform in file_info:\n",
    "    if inform.size < 1024 * 1024:  \n",
    "        size_group['маленькие'].append(inform)\n",
    "    elif inform.size <= 100 * 1024 * 1024: \n",
    "        size_group['средние'].append(inform)\n",
    "    else:  \n",
    "        size_group['большие'].append(inform)\n",
    "print(\"Статистика по размерам:\")\n",
    "for category, files in size_group.items():\n",
    "    print(f\"{category}: {len(files)} файлов\")\n",
    "files_deque: deque[str]  = deque(maxlen=10)\n",
    "for inform in file_info:\n",
    "    files_deque.append(inform.name + inform.extension)\n",
    "print(\"Последние 10 найденных файлов:\")\n",
    "for f in files_deque:\n",
    "    print(f)\n",
    "print(\"Размер списка file_info:\", sys.getsizeof(file_info), \"байт\")\n",
    "total: int  = 0\n",
    "for inform in file_info:\n",
    "    total += sys.getsizeof(inform)\n",
    "print(\"Суммарный размер всех FileInfo объектов:\", total, \"байт\")\n",
    "print(\"Всего:\", sys.getsizeof(file_info) + total, \"байт\")\n",
    "data = [inform._asdict() for inform in file_info]\n",
    "with open(\"results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbf7ad9",
   "metadata": {},
   "source": [
    "**Задача 3: Система конфигурации приложения**\n",
    "\n",
    "Создайте систему конфигурации используя ChainMap и defaultdict:\n",
    "\n",
    "1. Создайте namedtuple `Config` с полями: key, value, section, default_value\n",
    "2. Создайте несколько словарей конфигурации (default, user, environment)\n",
    "3. Используйте `ChainMap` для объединения конфигураций с приоритетом\n",
    "4. Используйте `defaultdict(dict)` для группировки настроек по секциям\n",
    "5. Используйте `OrderedDict` для сохранения порядка загрузки конфигураций\n",
    "6. Используйте `os.environ` для чтения переменных окружения\n",
    "7. Сериализуйте конфигурацию в JSON и pickle форматы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21929d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, ChainMap, defaultdict, OrderedDict\n",
    "from typing import Any\n",
    "import os\n",
    "import json\n",
    "Config = namedtuple('Config',['key', 'value', 'section', 'default_value']) #namedtuple `Config` с полями: key, value, section, default_value\n",
    "default: dict[str, Any] = {'theme':'none', 'language': 'ru'}\n",
    "user: dict[str, Any] = {'theme':'dark', 'language': 'en'}\n",
    "environment: dict[str, Any] = {'theme':'light', 'language': 'fr'}\n",
    "config_chain: ChainMap = ChainMap(environment, user, default) #`ChainMap` для объединения конфигураций с приоритетом\n",
    "section_config: defaultdict[str, dict[str, Any]] = defaultdict (dict) #`defaultdict(dict)` для группировки настроек по секциям\n",
    "key_to_section: dict[str, str] = {'theme': 'UI', 'language': 'General'}\n",
    "for key, section in key_to_section.items():\n",
    "    section_config[section][key] = config_chain[key]\n",
    "od_section_config: OrderedDict[str, dict] = OrderedDict()\n",
    "for section, settings in section_config.items():\n",
    "    od_section_config[section] = OrderedDict(settings) #для каждой секции свой OrderedDict\n",
    "config_chain = ChainMap(os.environ, environment, user, default)\n",
    "data : dict[str, dict[str, Any]] = {section : dict(settings) for section, settings in od_section_config.items()}\n",
    "with open('result.json', 'w') as file:\n",
    "    json.dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfc52c0",
   "metadata": {},
   "source": [
    "**Задача 4: Мониторинг системы**\n",
    "\n",
    "Создайте систему мониторинга используя sys и os:\n",
    "\n",
    "1. Создайте namedtuple `SystemInfo` с полями: cpu_count, memory_usage, process_id, user_name\n",
    "2. Используйте `os.cpu_count()` для получения количества процессоров\n",
    "3. Используйте `sys.getallocatedblocks()` для мониторинга памяти\n",
    "4. Используйте `os.getpid()` и `os.getlogin()` для информации о процессе\n",
    "5. Используйте `deque` для хранения последних 20 измерений\n",
    "6. Используйте `Counter` для подсчета частоты использования различных функций\n",
    "7. Используйте `defaultdict(list)` для группировки измерений по времени\n",
    "8. Сохраните историю мониторинга в pickle файл\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d32353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество процессоров в системе: 6\n",
      "мониторинг памяти: 355516\n",
      "deque([SystemInfo(cpu_count=6, memory_usage=355516, process_id=27000, user_name='User')], maxlen=20)\n",
      "defaultdict(<class 'list'>, {'09:10:04': [SystemInfo(cpu_count=6, memory_usage=355516, process_id=27000, user_name='User')]})\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple, deque, Counter, defaultdict, OrderedDict\n",
    "from typing import Any\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "SystemInfo = namedtuple('SystemInfo',['cpu_count', 'memory_usage', 'process_id', 'user_name']) #namedtuple `SystemInfo` с полями: cpu_count, memory_usage, process_id, user_name\n",
    "number_processes : int | None = os.cpu_count() #`os.cpu_count()` для получения количества процессоров\n",
    "print(\"Количество процессоров в системе:\", number_processes)\n",
    "memory: int = sys.getallocatedblocks() #`sys.getallocatedblocks()` для мониторинга памяти\n",
    "print(\"мониторинг памяти:\", memory)\n",
    "pid: int = os.getpid() #`os.getpid()` и `os.getlogin()` для информации о процессе\n",
    "login: str = os.getlogin()\n",
    "last_measurements: deque[SystemInfo] = deque(maxlen=20) # `deque` для хранения последних 20 измерений\n",
    "system_info: SystemInfo = SystemInfo(cpu_count=number_processes, memory_usage=memory, process_id=pid, user_name=login)\n",
    "last_measurements.append(system_info) #для хранения последних 20 измерений\n",
    "print(last_measurements)\n",
    "measurements_counter: Counter[str] = Counter()\n",
    "measurements_counter['get_system_info'] +=1\n",
    "time_group: defaultdict[str, list[SystemInfo]]=defaultdict(list)\n",
    "current_time = datetime.now().strftime('%H:%M:%S')\n",
    "time_group[current_time].append(system_info)\n",
    "print(time_group)\n",
    "\n",
    "with open('monitoring_history.pkl', 'wb') as f:\n",
    "    pickle.dump(last_measurements, f)\n",
    "with open('time_group.pkl', 'wb') as f:\n",
    "    pickle.dump(time_group, f)\n",
    "with open('measurements_counter.pkl', 'wb') as f:\n",
    "    pickle.dump(measurements_counter, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8870b90",
   "metadata": {},
   "source": [
    "**Задача 5: Система логирования**\n",
    "\n",
    "Создайте систему логирования используя все изученные коллекции:\n",
    "\n",
    "1. Создайте namedtuple `LogEntry` с полями: timestamp, level, message, module, function\n",
    "2. Используйте `deque` для хранения последних 100 логов (кольцевой буфер)\n",
    "3. Используйте `defaultdict(list)` для группировки логов по уровням (DEBUG, INFO, WARNING, ERROR)\n",
    "4. Используйте `Counter` для подсчета количества логов каждого уровня\n",
    "5. Используйте `OrderedDict` для хранения логов по времени (FIFO)\n",
    "6. Используйте `ChainMap` для объединения различных источников логов\n",
    "7. Используйте `os.path` для работы с файлами логов\n",
    "8. Сериализуйте логи в JSON и pickle форматы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554dfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Последний лог: LogEntry(timestamp='2025-09-22T09:47:08.483', level='INFO', message='Система запущена', module='__main__', function='main')\n",
      "Логи по уровням: defaultdict(<class 'list'>, {'INFO': [LogEntry(timestamp='2025-09-22T09:47:08.483', level='INFO', message='Система запущена', module='__main__', function='main')]})\n",
      "Счётчики уровней: Counter({'INFO': 1})\n",
      "Логи по времени (FIFO): OrderedDict([('2025-09-22T09:47:08.483', LogEntry(timestamp='2025-09-22T09:47:08.483', level='INFO', message='Система запущена', module='__main__', function='main'))])\n",
      "путь к файлу лога: c:\\Users\\User\\OneDrive\\Рабочий стол\\dev\\Python\\app_logs.json\n",
      "Файл не найден, создаём новый\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple, deque, Counter, defaultdict, OrderedDict, ChainMap\n",
    "from typing import Any\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "LogEntry = namedtuple('LogEntry',['timestamp', 'level', 'message', 'module', 'function']) #namedtuple `LogEntry`\n",
    "last_log: deque = deque(maxlen=100) \n",
    "logs_by_level: defaultdict[str, list[LogEntry]] = defaultdict(list)   # defaultdict(list) для группировки логов по уровням\n",
    "level_counts: Counter[str] = Counter()            # Counter для подсчёта логов каждого уровня\n",
    "entry:LogEntry = LogEntry(\n",
    "    timestamp = datetime.now().isoformat(timespec='milliseconds'),\n",
    "    level='INFO',\n",
    "    message='Система запущена',\n",
    "    module=__name__,\n",
    "    function='main'\n",
    ")\n",
    "last_log.append(entry)# Добавляем в кольцевой буфер\n",
    "logs_by_level[entry.level].append(entry)# Добавляем в defaultdict по уровню\n",
    "level_counts[entry.level] += 1 # Увеличиваем счетчик Counter\n",
    "print(\"Последний лог:\", last_log[-1])        # последний элемент в кольцевом буфере\n",
    "print(\"Логи по уровням:\", logs_by_level)     # сгруппированные по уровню\n",
    "print(\"Счётчики уровней:\", level_counts)     # количество по каждому уровню\n",
    "log_time: OrderedDict[str, dict] = OrderedDict()\n",
    "log_time[entry.timestamp] = entry\n",
    "print(\"Логи по времени (FIFO):\", log_time)\n",
    "log_source: ChainMap = ChainMap(logs_by_level, log_time) # `ChainMap` для объединения различных источников логов\n",
    "log_filename = 'app_logs.json'\n",
    "log_path = os.path.abspath(log_filename)# Получаем путь к файлу\n",
    "print(\"путь к файлу лога:\", log_path)\n",
    "if os.path.exists(log_path): # Проверяем, существует ли файл\n",
    "    print(\"Файл существует\")\n",
    "else:\n",
    "    print(\"Файл не найден, создаём новый\")\n",
    "with open('last_log.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump([entry._asdict() for entry in last_log], f, ensure_ascii=False, indent=4)\n",
    "with open('log_time.pkl', 'wb') as f:\n",
    "    pickle.dump(log_time, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf575634",
   "metadata": {},
   "source": [
    "**Задача 6: Кэш-система**\n",
    "\n",
    "Создайте простую кэш-систему используя collections:\n",
    "\n",
    "1. Создайте namedtuple `CacheEntry` с полями: key, value, timestamp, access_count\n",
    "2. Используйте `OrderedDict` для реализации LRU (Least Recently Used) кэша\n",
    "3. Используйте `deque` для хранения истории доступа к ключам\n",
    "4. Используйте `Counter` для подсчета частоты доступа к каждому ключу\n",
    "5. Используйте `defaultdict(int)` для хранения счетчиков доступа\n",
    "6. Реализуйте методы: get, set, delete, clear, size\n",
    "7. Используйте `sys.getsizeof()` для мониторинга размера кэша\n",
    "8. Сериализуйте кэш в pickle формат для сохранения между сессиями\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03141c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque, Counter, defaultdict, OrderedDict\n",
    "from typing import Any\n",
    "from datetime import datetime\n",
    "import os, pickle, sys\n",
    "CacheEntry = namedtuple('CacheEntry',['key', 'value', 'timestamp', 'access_count']) #namedtuple `CacheEntry`\n",
    "entry: CacheEntry = CacheEntry('user1', 'data1', timestamp=datetime.now(), access_count=0)\n",
    "cache: OrderedDict[str, dict] = OrderedDict() #`OrderedDict` для реализации LRU (Least Recently Used) кэша\n",
    "cache[\"cache1\"]='001'\n",
    "cache[\"cache2\"]='002'\n",
    "cache[\"cache3\"]='003'\n",
    "history_key: deque = deque() #`deque` для хранения истории доступа к ключам\n",
    "history_key.append('cache2')\n",
    "history_key.append('cache3')\n",
    "access_counter = Counter(history_key) #`Counter` для подсчета частоты доступа к каждому ключу\n",
    "access_count_dict : defaultdict[str, int] = defaultdict(int)   # `defaultdict(int)` для хранения счетчиков доступа\n",
    "access_count_dict ['user1']+=1\n",
    "def set_value(key: str, value: Any):\n",
    "    entry = CacheEntry(key, value, datetime.now(), 0)# создаём запись\n",
    "    cache[key] = entry # добавляем/обновляем в кэше\n",
    "    cache.move_to_end(key)# перемещаем в конец для LRU\n",
    "    history_key.append(key)# добавляем в историю\n",
    "    access_count_dict[key] += 1 # увеличиваем счётчик доступа\n",
    "def get_value(key: str, default=None):\n",
    "    if key in cache:         \n",
    "        cache.move_to_end(key)# обновляем LRU\n",
    "        history_key.append(key)# добавляем в историю\n",
    "        access_count_dict[key] += 1 # увеличиваем счётчик доступа\n",
    "        return cache[key].value # возвращаем значение\n",
    "    else:\n",
    "        return default\n",
    "def delete_value(key: str):\n",
    "    if key in cache:\n",
    "        removed = cache.pop(key)  # удаляем из кэша\n",
    "        history_key.append(key)   # фиксируем удаление в истории\n",
    "        return removed.value\n",
    "    else:\n",
    "        return None\n",
    "def clear_cache():\n",
    "    cache.clear()               # очищаем LRU-кэш\n",
    "    history_key.clear()         # очищаем историю\n",
    "    access_count_dict.clear()   # очищаем defaultdict\n",
    "    access_counter.clear()      # очищаем Counter\n",
    "def cache_size():\n",
    "    size = sys.getsizeof(cache)# сколько памяти занимает сам объект cache\n",
    "    for key, entry in cache.items(): # проходим по всем элементам кэша: key, entry\n",
    "        size += sys.getsizeof(key) # добавляем размер ключа\n",
    "        size += sys.getsizeof(entry) # добавляем размер записи CacheEntry\n",
    "    return size # общий размер кэша в байтах   \n",
    "with open(\"cache.pkl\", \"wb\") as f:   \n",
    "    pickle.dump(cache, f) # сохраняем кэш\n",
    "if os.path.exists(\"cache.pkl\"):       # проверяем, есть ли файл\n",
    "    with open(\"cache.pkl\", \"rb\") as f:  \n",
    "        cache = pickle.load(f)          # загружаем кэш"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e79981b",
   "metadata": {},
   "source": [
    "**Задача 7: Анализатор текста**\n",
    "\n",
    "Создайте анализатор текста используя collections:\n",
    "\n",
    "1. Создайте namedtuple `WordInfo` с полями: word, frequency, length, first_occurrence\n",
    "2. Используйте `Counter` для подсчета частоты слов\n",
    "3. Используйте `defaultdict(list)` для группировки слов по длине\n",
    "4. Используйте `deque` для хранения последних 50 уникальных слов\n",
    "5. Используйте `OrderedDict` для хранения слов в порядке первого появления\n",
    "6. Используйте `os.path` для работы с текстовыми файлами\n",
    "7. Используйте `sys.getsizeof()` для анализа памяти\n",
    "8. Сохраните результаты анализа в JSON файл\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2edbc2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер Counter: 248\n",
      "Размер defaultdict: 240\n",
      "Размер OrderedDict: 488\n",
      "Размер deque: 624\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple, deque, Counter, defaultdict, OrderedDict\n",
    "from typing import Any\n",
    "import json, sys\n",
    "WordInfo = namedtuple('WordInfo',['word', 'frequency', 'length', 'first_occurrence']) #amedtuple `WordInfo` с полями: word, frequency, length, first_occurrence\n",
    "word_1: WordInfo = WordInfo('hello', 3, 5, 'hello world')\n",
    "text = \"hello world hello python code python hello\" # Текст для анализа (вместо файла)\n",
    "words = text.split() # разбиение текста на слова\n",
    "counter = Counter(words) #`Counter` для подсчета частоты слов\n",
    "group_words: defaultdict[str, list[WordInfo]] = defaultdict(list) #для группировки слов по длине\n",
    "unic_words: deque = deque(maxlen=50) #`deque` для хранения последних 50 уникальных слов\n",
    "first_use: OrderedDict[str, dict] = OrderedDict() #`OrderedDict` для хранения слов в порядке первого появления\n",
    "for idx, word in enumerate(words):#Создание WordInfo для каждого слова и заполнение всех структур\n",
    "    if word not in first_use:\n",
    "        wi = WordInfo(word, counter[word], len(word), idx)\n",
    "        first_use[word] = wi\n",
    "        group_words[len(word)].append(wi)\n",
    "        unic_words.append(word) \n",
    "print(\"Размер Counter:\", sys.getsizeof(counter))\n",
    "print(\"Размер defaultdict:\", sys.getsizeof(group_words))\n",
    "print(\"Размер OrderedDict:\", sys.getsizeof(first_use))\n",
    "print(\"Размер deque:\", sys.getsizeof(unic_words))\n",
    "result = {\n",
    "    \"group_words\": {k: [wi._asdict() for wi in v] for k, v in group_words.items()},\n",
    "    \"first_use\": {k: v._asdict() for k, v in first_use.items()},\n",
    "    \"last_50_words\": list(unic_words)\n",
    "}\n",
    "with open(\"analysis.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f76b8",
   "metadata": {},
   "source": [
    "**Задача 8: Система управления задачами**\n",
    "\n",
    "Создайте систему управления задачами (TODO) используя все изученные концепции:\n",
    "\n",
    "1. Создайте namedtuple `Task` с полями: id, title, description, priority, status, created_date\n",
    "2. Используйте `defaultdict(list)` для группировки задач по статусу (todo, in_progress, done)\n",
    "3. Используйте `deque` для очереди задач с высоким приоритетом\n",
    "4. Используйте `Counter` для подсчета задач по приоритету\n",
    "5. Используйте `OrderedDict` для хранения задач в порядке создания\n",
    "6. Используйте `ChainMap` для объединения различных списков задач\n",
    "7. Используйте `os.path` для работы с файлами задач\n",
    "8. Реализуйте функции: add_task, complete_task, get_tasks_by_status, get_priority_queue\n",
    "9. Сериализуйте все данные в JSON и pickle форматы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa937938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque, Counter, defaultdict, OrderedDict\n",
    "from datetime import datetime\n",
    "import json\n",
    "Task = namedtuple('Task',['id', 'title', 'description', 'priority', 'status', 'created_date']) #namedtuple `Task` с полями: id, title, description, priority, status, created_date\n",
    "task_1: Task = Task('1', 'cook', 'cook dinner', '2', 'todo', datetime(2025, 9, 25))\n",
    "task_2: Task = Task('2', 'study', 'learn python', '1', 'todo', datetime(2025, 9, 25))\n",
    "group_status: defaultdict[str, list[Task]] = defaultdict(list) #для группировки задач по статусу (todo, in_progress, done)\n",
    "high_priority: deque = deque() #для очереди задач с высоким приоритетом\n",
    "counter_priority = Counter() # для подсчета задач по приоритету\n",
    "for i in [task_1, task_2]:\n",
    "    counter_priority[i.priority] += 1\n",
    "tasks: OrderedDict[str, dict] = OrderedDict() #хранение задач в порядке создания\n",
    "united_task: ChainMap = ChainMap(high_priority, group_status) # для объединения различных списков задач\n",
    "def add_task(task: Task):\n",
    "    tasks[task.id] = task# добавление в OrderedDict, чтобы сохранить порядок создания\n",
    "    group_status[task.status].append(task) # Добавляем в defaultdict по статусу\n",
    "    if task.priority == '1':# Добавляем в очередь высоких приоритетов (1 — высокий)\n",
    "        high_priority.append(task)\n",
    "    counter_priority[task.priority] += 1 # Обновляем Counter по приоритету\n",
    "def complete_task(task_id: str):\n",
    "    if task_id in tasks:\n",
    "        task = tasks[task_id]\n",
    "        new_task = Task(task.id, task.title, task.description, task.priority, 'done', task.created_date) # Меняем статус на done\n",
    "        tasks[task_id] = new_task\n",
    "        \n",
    "        group_status[task.status].remove(task) # Обновляем defaultdict\n",
    "        group_status['done'].append(new_task)\n",
    "        \n",
    "        if task in high_priority:# удаляем в очереди высоких приоритетов — \n",
    "            high_priority.remove(task)\n",
    "        \n",
    "        print(f\"Задача {task_id} выполнена.\")\n",
    "    else:\n",
    "        print(f\"Задача {task_id} не найдена.\")\n",
    "def get_tasks_by_status(status: str):\n",
    "    return group_status.get(status, [])\n",
    "def get_priority_queue():\n",
    "    return list(high_priority)\n",
    "with open(\"tasks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({k: t._asdict() for k, t in tasks.items()}, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(\"tasks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tasks, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37042a40",
   "metadata": {},
   "source": [
    "**Задача 9: Система мониторинга производительности**\n",
    "\n",
    "Создайте систему мониторинга производительности используя sys и collections:\n",
    "\n",
    "1. Создайте namedtuple `PerformanceMetric` с полями: function_name, execution_time, memory_usage, timestamp\n",
    "2. Используйте `deque` для хранения последних 100 измерений производительности\n",
    "3. Используйте `defaultdict(list)` для группировки метрик по функциям\n",
    "4. Используйте `Counter` для подсчета количества вызовов каждой функции\n",
    "5. Используйте `OrderedDict` для хранения метрик в хронологическом порядке\n",
    "6. Используйте `sys.getsizeof()` для мониторинга памяти\n",
    "7. Используйте `os.path` для работы с файлами метрик\n",
    "8. Реализуйте функции: record_metric, get_function_stats, get_memory_usage, export_metrics\n",
    "9. Сериализуйте метрики в JSON и pickle форматы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a0699ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque, Counter, defaultdict, OrderedDict\n",
    "import json, time, pickle, sys, os\n",
    "from typing import Optional\n",
    "\n",
    "PerformanceMetric = namedtuple(\"PerformanceMetric\", [\"function_name\", \"execution_time\", \"memory_usage\", \"timestamp\"]) #namedtuple `PerformanceMetric` с полями: function_name, execution_time, memory_usage, timestamp\n",
    "last_metrics: deque[PerformanceMetric]  = deque(maxlen=100)  #deque` для хранения последних 100 измерений производительности\n",
    "metrics_by_function: defaultdict[str, list[PerformanceMetric]] = defaultdict(list) #группировка метрик по функциям \n",
    "function_calls: Counter = Counter() #для подсчета количества вызовов каждой функции\n",
    "metrics_chronological: OrderedDict[float, PerformanceMetric] = OrderedDict() # для хранения метрик в хронологическом порядке\n",
    "if os.path.exists(\"metrics.json\"):\n",
    "    print(\"Файл существует\")\n",
    "def record_metric(function_name: str, execution_time: float) -> None: #запись метрики\n",
    "    timestamp: float = time.time()\n",
    "    metric = PerformanceMetric(\n",
    "        function_name=function_name,\n",
    "        execution_time=execution_time,\n",
    "        memory_usage=sys.getsizeof(execution_time),  # создание метрики с текущим временем\n",
    "        timestamp=timestamp\n",
    "    )\n",
    "    last_metrics.append(metric) #добавление в deque \n",
    "    metrics_by_function[function_name].append(metric) #группирует метркиу по функции \n",
    "    function_calls[function_name] += 1\n",
    "    metrics_chronological[timestamp] = metric #добавление для хронологии\n",
    "def get_function_stats(function_name: str) -> Optional[dict[str, float]]: #получение статистики\n",
    "    metrics = metrics_by_function.get(function_name, [])\n",
    "    if not metrics:\n",
    "        return None\n",
    "    total_time = sum(m.execution_time for m in metrics) #обще количество измерений\n",
    "    avg_time = total_time / len(metrics) #среднее время выполнения\n",
    "    return {\"total_calls\": len(metrics), \"average_time\": avg_time}\n",
    "def get_memory_usage()-> int: #получение памяти\n",
    "    return sum(sys.getsizeof(m) for m in last_metrics)\n",
    "def export_metrics() -> None:\n",
    "    with open(\"metrics.json\", \"w\") as f:\n",
    "        json.dump([m._asdict() for m in last_metrics], f, indent=2)\n",
    "    with open(\"metrics.pkl\", \"wb\") as f:\n",
    "        pickle.dump(list(last_metrics), f)\n",
    "if os.path.exists(\"metrics.json\"):\n",
    "    print(\"Файл metrics.json существует\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24490e6",
   "metadata": {},
   "source": [
    "**Задача 10: Комплексная система управления данными**\n",
    "\n",
    "Создайте комплексную систему управления данными, объединяющую все изученные концепции:\n",
    "\n",
    "1. Создайте несколько namedtuple для различных типов данных (User, Product, Order, etc.)\n",
    "2. Используйте `defaultdict` для создания индексов по различным полям\n",
    "3. Используйте `deque` для реализации очередей обработки данных\n",
    "4. Используйте `Counter` для аналитики и статистики\n",
    "5. Используйте `OrderedDict` для хранения данных в определенном порядке\n",
    "6. Используйте `ChainMap` для объединения различных источников данных\n",
    "7. Используйте `os` и `sys` для работы с файловой системой и мониторинга\n",
    "8. Реализуйте CRUD операции (Create, Read, Update, Delete)\n",
    "9. Добавьте функции экспорта/импорта данных в различных форматах\n",
    "10. Сериализуйте все данные в JSON, pickle и другие форматы\n",
    "11. Добавьте типизацию для всех функций и классов\n",
    "12. Реализуйте систему логирования для отслеживания операций\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015343c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict, deque, Counter, OrderedDict, ChainMap\n",
    "import json, pickle, os, sys, logging\n",
    "from typing import Optional, Any\n",
    "#namedtuple для различных типов данных (User, Product, Order, etc.)\n",
    "User = namedtuple(\"User\", [\"user_id\", \"name\", \"email\"])\n",
    "Product = namedtuple(\"Product\", [\"product_id\", \"name\", \"price\"])\n",
    "Order = namedtuple(\"Order\", [\"order_id\", \"user_id\", \"product_id\", \"quantity\"])\n",
    "users: dict[int, User] = {}\n",
    "products: dict[int, Product] = {}\n",
    "orders: OrderedDict[int, Order] = OrderedDict()\n",
    "\n",
    "# Индексы по различным полям\n",
    "users_by_email: defaultdict[str, list[User]] = defaultdict(list)\n",
    "products_by_name: defaultdict[str, list[Product]] = defaultdict(list)\n",
    "\n",
    "order_queue: deque[Order] = deque() # Очередь обработки данных\n",
    "order_count: Counter = Counter() # для аналитики и статистики\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\") #вывод логов для отслеживания всех операций\n",
    "\n",
    "def create_user(user_id: int, name: str, email: str) -> User:\n",
    "    user = User(user_id, name, email)\n",
    "    users[user_id] = user\n",
    "    users_by_email[email].append(user)\n",
    "    logging.info(f\"User created: {user}\")\n",
    "    return user\n",
    "\n",
    "def create_product(product_id: int, name: str, price: float) -> Product:\n",
    "    product = Product(product_id, name, price)\n",
    "    products[product_id] = product\n",
    "    products_by_name[name].append(product)\n",
    "    logging.info(f\"Product created: {product}\")\n",
    "    return product\n",
    "\n",
    "def create_order(order_id: int, user_id: int, product_id: int, quantity: int) -> Order:\n",
    "    order = Order(order_id, user_id, product_id, quantity)\n",
    "    orders[order_id] = order\n",
    "    order_queue.append(order)\n",
    "    order_count[product_id] += quantity\n",
    "    logging.info(f\"Order created: {order}\")\n",
    "    return order\n",
    "\n",
    "#получение пользователя по айди, продукта по имени и заказа по айди\n",
    "def get_user_by_email(email: str) -> list[User]:\n",
    "    return users_by_email.get(email, [])\n",
    "\n",
    "def get_product_by_name(name: str) -> list[Product]:\n",
    "    return products_by_name.get(name, [])\n",
    "\n",
    "def get_order(order_id: int) -> Optional[Order]:\n",
    "    return orders.get(order_id)\n",
    "\n",
    "def update_product_price(product_id: int, new_price: float) -> None:\n",
    "    if product_id in products:\n",
    "        product = products[product_id]\n",
    "        updated_product = Product(product.product_id, product.name, new_price)\n",
    "        products[product_id] = updated_product\n",
    "        logging.info(f\"Product updated: {updated_product}\")\n",
    "def delete_user(user_id: int) -> None:\n",
    "    if user_id in users:\n",
    "        user = users.pop(user_id)\n",
    "        users_by_email[user.email].remove(user)\n",
    "        logging.info(f\"User deleted: {user}\")\n",
    "\n",
    "def export_data() -> None:\n",
    "    with open(\"users.json\", \"w\") as f:\n",
    "        json.dump([u._asdict() for u in users.values()], f)\n",
    "    with open(\"data.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"users\": users, \"products\": products, \"orders\": orders}, f)\n",
    "    logging.info(\"Data exported to JSON and Pickle\")\n",
    "\n",
    "def import_data() -> None:\n",
    "    global users, products, orders\n",
    "    if os.path.exists(\"data.pkl\"):\n",
    "        with open(\"data.pkl\", \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            users = data[\"users\"]\n",
    "            products = data[\"products\"]\n",
    "            orders = data[\"orders\"]\n",
    "        logging.info(\"Data imported from Pickle\")\n",
    "\n",
    "combined_data = ChainMap(users, products) #для объединения данных\n",
    "print(f\"Использовано памяти: {sys.getsizeof(users)} bytes\")\n",
    "if not os.path.exists(\"exports\"):\n",
    "    os.mkdir(\"exports\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
